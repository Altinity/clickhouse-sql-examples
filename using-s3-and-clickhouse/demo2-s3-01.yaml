apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "demo2"
spec:
  configuration:
    clusters:
      - name: "s3"
        layout:
          replicasCount: 2
          shardsCount: 1
        templates:
          podTemplate: replica-1
          volumeClaimTemplate: storage
    zookeeper:
        nodes:
        - host: keeper-keeper
          port: 2181
    profiles:
      default/allow_experimental_refreshable_materialized_view: 1
    files:
      # Allow zero-copy replication for demonstration purposes. It
      # should be used very cautiously in production settings. 
      config.d/ed_zero_copy.xml: |
        <clickhouse>
           <merge_tree>
             <allow_remote_fs_zero_copy_replication>false</allow_remote_fs_zero_copy_replication>
           </merge_tree>
        </clickhouse>
      config.d/ed_s3_datalake.xml: |
        <clickhouse>
          <s3>
            <data-lake>
              <endpoint from_env="AWS_S3_DATALAKE_URL"/>
              <use_environment_credentials>1</use_environment_credentials>
            </data-lake>
          </s3>
        </clickhouse>
      config.d/ed_storage_configuration.xml: |
        <clickhouse>
          <storage_configuration>
            <disks>
              <!-- Disk with single endpoint for all replicas. -->
              <s3_disk>
                <type>s3</type>
                <endpoint from_env="AWS_S3_DISK_URL"/>
                <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>
                <use_environment_credentials>1</use_environment_credentials>
              </s3_disk>
              <!-- Disk with separate endpoint for each replica. -->
              <s3_disk_with_replica>
                <type>s3</type>
                <endpoint from_env="AWS_S3_DISK_WITH_REPLICA_URL"/>
                <metadata_path>/var/lib/clickhouse/disks/s3_disk_with_replica/</metadata_path>
                <use_environment_credentials>1</use_environment_credentials>
              </s3_disk_with_replica>
              <!-- Disk with single endpoint for each shard. -->
              <s3_zero_copy>
                <type>s3</type>
                <endpoint from_env="AWS_S3_ZERO_COPY_URL"/>
                <metadata_path>/var/lib/clickhouse/disks/s3_zero_copy/</metadata_path>
                <use_environment_credentials>1</use_environment_credentials>
              </s3_zero_copy>
              <!-- Local disk cache -->
              <s3_disk_cache>
                <type>cache</type>
                <disk>s3_disk</disk>
                <path>/var/lib/clickhouse/s3_disk_cache/cache/</path>
                <max_size>100Gi</max_size>
                <cache_on_write_operations>1</cache_on_write_operations>
              </s3_disk_cache>
            </disks>
            <policies>
              <!-- S3-backed MergeTree with single endpoint, no disk cache -->
              <s3_disk_policy>
                <volumes>
                  <main>
                    <disk>s3_disk</disk>
                  </main>
                </volumes>
              </s3_disk_policy>
              <!-- S3-backed MergeTree with an endpoint for each replica, no disk cache -->
              <s3_disk_with_replica_policy>
                <volumes>
                  <main>
                    <disk>s3_disk_with_replica</disk>
                  </main>
                </volumes>
              </s3_disk_with_replica_policy>
              <!-- S3-backed MergeTree with disk cache -->
              <s3_disk_cached_policy>
                <volumes>
                  <main>
                    <disk>s3_disk_cache</disk>
                  </main>
                </volumes>
              </s3_disk_cached_policy>
              <!-- Block storage tiered with S3-backed MergeTree -->
              <s3_tiered_policy>
                <volumes>
                  <default>
                    <disk>default</disk>
                    <move_factor>0.1</move_factor>
                  </default>
                  <s3_disk_cached>
                      <disk>s3_disk_cache</disk>
                      <prefer_not_to_merge>true</prefer_not_to_merge>
                      <perform_ttl_move_on_insert>false</perform_ttl_move_on_insert>
                  </s3_disk_cached>
                </volumes>
              </s3_tiered_policy>
              <!-- S3-backed MergeTree with zero-copy replication -->
              <s3_zero_copy_policy>
                <volumes>
                  <main>
                    <disk>s3_zero_copy</disk>
                  </main>
                </volumes>
              </s3_zero_copy_policy>
            </policies>
          </storage_configuration>
        </clickhouse>
  templates:
    podTemplates:
      - name: replica-1
        spec:
          nodeSelector:
            node.kubernetes.io/instance-type: m6i.large
          containers:
          - name: clickhouse
            image: clickhouse/clickhouse-server:24.3.3.102
            env:
            - name: AWS_S3_DISK_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_S3_DISK_URL
            - name: AWS_S3_DISK_WITH_REPLICA_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_S3_DISK_WITH_REPLICA_URL
            - name: AWS_S3_ZERO_COPY_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_S3_ZERO_COPY_URL
            - name: AWS_S3_DATALAKE_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_S3_DATALAKE_URL
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_SECRET_ACCESS_KEY
    volumeClaimTemplates:
      - name: storage
        # PVCs must be deleted manually.
        reclaimPolicy: Retain
        spec:
          storageClassName: gp3-encrypted
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 200Gi
