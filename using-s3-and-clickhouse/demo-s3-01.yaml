apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "demo"
spec:
  configuration:
    clusters:
      - name: "s3"
        layout:
          replicas:
          - templates:
              podTemplate: replica-1
              volumeClaimTemplate: storage
          shardsCount: 1
        templates:
    files:
      config.d/ed_s3.xml: |
        <clickhouse>
          <s3>
            <data-lake>
              <endpoint from_env="AWS_S3_DATALAKE_URL"/>
              <use_environment_credentials>1</use_environment_credentials>
            </data-lake>
          </s3>
        </clickhouse>
      config.d/ed_storage.xml: |
        <clickhouse>
          <storage_configuration>
            <disks>
              <!-- Disk with single endpoint for all replicas. -->
              <s3_disk>
                <type>s3</type>
                <endpoint from_env="AWS_S3_DISK_URL"/>
                <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>
                <use_environment_credentials>1</use_environment_credentials>
              </s3_disk>
              <s3_disk_cache>
                <type>cache</type>
                <disk>s3_disk</disk>
                <path>/var/lib/clickhouse/s3_disk_cache/cache/</path>
                <max_size>100Gi</max_size>
                <cache_on_write_operations>1</cache_on_write_operations>
              </s3_disk_cache>
            </disks>
            <policies>
              <!-- S3-backed MergeTree with single endpoint, no disk cache -->
              <s3_disk_policy>
                <volumes>
                  <main>
                    <disk>s3_disk</disk>
                  </main>
                </volumes>
              </s3_disk_policy>
              <!-- S3-backed MergeTree with disk cache -->
              <s3_disk_cache_policy>
                <volumes>
                  <main>
                    <disk>s3_disk_cache</disk>
                  </main>
                </volumes>
              </s3_disk_cache_policy>
              <!-- Block storage tiered with S3-backed MergeTree -->
              <s3_tiered_policy>
                <volumes>
                  <default>
                    <disk>default</disk>
                    <move_factor>0.1</move_factor>
                  </default>
                  <s3_disk_cache>
                      <disk>s3_disk_cache</disk>
                      <prefer_not_to_merge>true</prefer_not_to_merge>
                      <perform_ttl_move_on_insert>false</perform_ttl_move_on_insert>
                  </s3_disk_cache>
                </volumes>
              </s3_tiered_policy>
            </policies>
          </storage_configuration>
        </clickhouse>
  templates:
    podTemplates:
      - name: replica-1
        spec:
          nodeSelector:
            node.kubernetes.io/instance-type: m6i.xlarge
          containers:
          - name: clickhouse
            image: clickhouse/clickhouse-server:24.3.2.23
            env:
            - name: AWS_S3_DISK_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_S3_DISK_URL
            - name: AWS_S3_DATALAKE_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_S3_DATALAKE_URL
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: AWS_SECRET_ACCESS_KEY
    volumeClaimTemplates:
      - name: storage
        # reclaimPolicy: Retain
        spec:
          storageClassName: gp3-encrypted
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 200Gi
